{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931db05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f74054",
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b421e9",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for NASA's news\n",
    "url =\"https://redplanetscience.com/\"\n",
    "# open the url\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a975e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest news headline and paragrapgh\n",
    "news_title = soup.find_all('div', class_='content_title')[0].text\n",
    "news_p = soup.find_all('div', class_='article_teaser_body')[0].text\n",
    "print(news_title)\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0f8e7",
   "metadata": {},
   "source": [
    "## JPL Mars Space Imagesâ€”Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0477e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for Mars' image\n",
    "jpl_url = 'https://spaceimages-mars.com/'\n",
    "\n",
    "# open the jpl_url\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# find the image\n",
    "featured_image_url = soup.find_all('img')[2][\"src\"]\n",
    "\n",
    "# display larger size image\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the url for the image\n",
    "image = jpl_url + featured_image_url\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42215d5",
   "metadata": {},
   "source": [
    "## Mars Facts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for NASA's mars facts\n",
    "facts_url = 'https://galaxyfacts-mars.com/'\n",
    "\n",
    "# parse the url\n",
    "table = pd.read_html(facts_url)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98087c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to df and renaming columns\n",
    "facts = table[0]\n",
    "facts.columns=['Description', 'Mars', 'Earth']\n",
    "facts = facts.drop([0])\n",
    "facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to html\n",
    "fact_table = facts.to_html()\n",
    "fact_table.replace('\\n','')\n",
    "print(fact_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d39b9",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for Mars' Hemispheres\n",
    "hems_url = 'https://marshemispheres.com/'\n",
    "\n",
    "# open the jpl_url\n",
    "browser.visit(hems_url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ce3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to html and extract image urls\n",
    "hems_data = soup.find_all(\"div\", class_=\"item\")\n",
    "\n",
    "hemisphere_img_urls = []\n",
    "\n",
    "# loop through the data to find title and url info\n",
    "for image in hems_data:\n",
    "    \n",
    "    title = image.find(\"h3\").text\n",
    "    \n",
    "    img_url = image.a[\"href\"]\n",
    "    \n",
    "    url = hems_url + img_url\n",
    "    \n",
    "    # use requests to get full images url \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # create soup object\n",
    "    soup = bs(response.text,\"html.parser\")\n",
    "    \n",
    "    # find full image url\n",
    "    new_url = soup.find(\"img\", class_=\"wide-image\")[\"src\"]\n",
    "    \n",
    "    # create full image url\n",
    "    full_url = \"https://marshemispheres.com/\" + new_url\n",
    "    \n",
    "   \n",
    "    # create a dictionary and append to the list before the loop\n",
    "    hemisphere_img_urls.append({\"title\": title, \"img_url\": full_url})\n",
    "\n",
    "hemisphere_img_urls    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData38]",
   "language": "python",
   "name": "conda-env-PythonData38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
